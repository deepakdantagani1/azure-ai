/**
 * Mitigate potential harms techniques can be applied at each of four layers
 * 
 * ==> Model
 * ==> Safety System
 * ==> Metaprompt and grounding
 * ==> User experience
 * 
 */

/**
 * Model layer 
 * ==> Selecting a model that is appropriate for the intended solution use
 * ==> Fine-tuning a foundational model with your own training data so that the responses it generates are more likely to be 
 * relevant and scoped to your solution scenario.
 */

/**
 * The safety system layer
 * ==> Azure OpenAI Service includes support for content filters 
 * ==> abuse detection algorithms
 */

/**
 * The metaprompt and grounding layer
 * ==> Specifying metaprompts or system inputs that define behavioral parameters for the model.
 * ==> Applying prompt engineering to add grounding data to input prompts
 * ==> Using a retrieval augmented generation (RAG) approach to retrieve contextual data from
 *  trusted data sources and include it in prompts.
 */

/**
 * The user experience layer
 * ==> Documentation
 * ==> user input validations
 */